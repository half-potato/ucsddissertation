\section{Introduction}
\label{sec:intro}
The field of 3D reconstruction for novel view synthesis has explored a variety of scene representations: point based~\cite{kerbl20233d}, surface based~\cite{li2023neuralangelo, yariv2023bakedsdf, wang2021neus}, and volume based~\cite{mildenhall2020nerf, muller2022instant}. Since their introduction in NeRF~\cite{mildenhall2020nerf}, differentiable rendering of volumetric scene representations have become popular due to their ability to yield photorealistic 3D reconstructions.
More recently, 3D Gaussian Splatting (3DGS) combined the speed of point based models with the differentiability of volume based representations by representing the scene as a collection of millions of Gaussian primitives that can be rendered via rasterization in real-time.

Unlike NeRF, 3DGS lacks a true volumetric density field, and uses Gaussians to describe the \emph{opacity} of the scene rather than of density.
As such, 3DGS's scene representation isn't volumetric rendering, as an anisotropic Gaussian primitive in 3DGS will appear to have the same opacity regardless of the viewing direction of the camera.
This lack of a consistent underlying density field prohibits the use of various algorithms and regularizers (such as the distortion loss from Mip-NeRF360~\cite{barron2021mip}), but the biggest downside of this model is popping.
Popping in 3DGS is due to two assumptions made by the model: that primitives do not overlap, and that (given a camera position) primitives can be sorted accurately using only their centers. These assumptions are almost always violated in practice, which causes the rendered image to change significantly as the camera moves due to the sort-order of primitives changing.
This popping may not be noticeable when primitives are small, but describing a large scene using many small primitives requires a prohibitively large amount of memory.

In this work we build on the primitive based representation of 3DGS, but introduce a method that allows for the physically accurate blending of an infinite number of overlapping number of primitives, specifically constant density ellipsoids.
We implement this blending method in a ray-tracing framework, which allows us to model various optical effects like radial distortion lenses including fisheye and defocus blur in a straightforward way, all at real-time framerates.
Our method guarantees 3D consistent real-time rendering while also improving the image quality of our 3DGS baselines. This is particularly pronounced in the challenging large-scale Zip-NeRF scenes~\cite{barron2023zip} where our method matches the quality of state-of-the-art offline rendering methods.

% In summary, our contributions are:
% \begin{enumerate}
%   \item Suggesting a mixture of ellipsoidal primitives with constant density for differentiable volume rendering.
%   \item An accurate computation of the volume rendering integral given a set of the aforementioned primitives which guarantees pop-free rendering.
%   \item Highest quality results on the hardest datasets like Zip-NeRF apartments across real-time rendering methods. 
% \end{enumerate}


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/splatting_vs_volume_rendering.pdf}
%     \caption{Left: Depiction of Gaussian alpha field sliced to produce image. Right: Depiction of a Gaussian density field volume rendered to produce image.}
%     \label{fig:splatting_vs_volume_rendering}
% \end{figure}

%In Computer Graphics the two most prominent techniques to depict 3D assets on screen is Rasterization and Ray-Tracing. Rasterization is often referred to the traditional hardware accelerated pipeline where primitives, often triangles, quads or points are being projected in the 2D image plane. On the contrary, ray-tracing, describes the process of shooting rays through pixels and computing the primitives with which rays intersect. One can see one as the opposite process of the other. While the two methods were roughly developed during the same time, rasterization quickly became the most popular technique for real-time graphics, because of the low computational requirements. Eventually hardware was specifically developed to accelerate it even more. On the contrary, ray tracing is mostly used when accuracy is more important than speed i.e VfX, architectural renderings etc. The generality of ray-tracing allows to model non-local effects more accurately. Often Ray Tracing still runs in CPUs instead of GPUs, although recently this trend is quickly changing through the introduction of OpTiX and other GPU pipelines.

%The introduction of Neural Radiance Fields (NeRF) has sparked a recent surge of interest in 3D reconstruction and novel view synthesis. NeRF as a 3D representation is a differentiable volumetric radiance field. During rendering, NeRF use the ray-tracing paradigm by casting rays and evaluating the volumetric rendering equation to compute the colors of the pixels. It is predominantly a ray-tracing technique which allows for an accurate evaluation of the volumetric integral\cite{}. The term "NeRF" extends beyond the image formation model to encompass the underlying representation of radiance as a continuous field in $R^3$, where each position is associated with a density and color.

%Most recently 3D Gaussian Splatting (3DGS) proposed a rasterization based alternative to NeRF that achieves unprecedented quality/performance trade-off. 3DGS differentiably rasterizes a set of Gaussian primitives in screen. The Gaussian means and covariance matrices are projected in screen space, sorted based on depth and alpha blended front-to-back. This process is a fast approximation of the volumetric rendering equation. 

%This process closely approximates volumetric rendering, with the following caveats:
%\begin{enumerate}
%\item Self-shadowing is neglected.
%\item Primitive overlap is disregarded.
%\item Instead of integrating density along the z-axis to derive opacity, 3DGS directly models opacity by slicing the Gaussian kernel.
%\item Primitives are assumed to be accurately sortable by their mean.
%\end{enumerate}

%This process closely resembles volumetric rendering, but with the following important approximations: 1) It doesn't account for self-shadowing, meaning that given a single primitive the attenuation of transparency along the traversed volume is ignored. Secondly, it overlooks primitive overlap, by enforcing a strict order of the primitives and rendering them as if all the density of one is strictly in front of the other, this not only doesn't allow for proper color mixing of density, but  also introduces temporal artifacts also referred to as popping~\cite{radl2024stopthepop}. Furthermore, it operates under the assumption that primitives can be accurately sorted by their mean values. Finally, instead of integrating density along the z-axis to determine opacity, 3DGS directly models opacity by slicing the Gaussian kernel. 

%The efficiency and accuracy of 3DGS comes from the combination of the primitive-based scene representation combined with the fast rasterization-based rendering. In this paper we re-use the primitive-based representation of 3DGS but we explore a rendering framework based on ray-tracing that evaluates accurately the volumetric rendering equation by lifting the approximations mentioned above. \GK{Lets add a few sentences here on the main features of our framework 1) we use optix 2) we solve challenges with the rendering using this or that etc}

%We use our rendering framework as a drop-in replacement to 3DGS code-base and we show that it improves overall quality in most metrics and scenes. It is also the first primitive-based method that is guaranteed to completely remove popping artifacts. We also show that the generality of the ray-tracing paradigm not only allows us to accurately compute the volume integral but also enables us to model complex camera models with radial distortion, efficiently render depth of field and other complex camera effects and add synthetic reflective and transparent objects in the scene.

%In summary our contribution are