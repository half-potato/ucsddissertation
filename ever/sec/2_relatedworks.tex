\section{Related Work}

\myparagraph{Neural Volume Rendering} Neural Radiance Fields (NeRF)~\cite{mildenhall2020nerf} introduced the paradigm of representing a scene as an emissive volume, using a neural network with position encoding that is rendered differentiably using quadrature~\cite{max1995optical, drebin1988volume} and optimized using gradient descent. 
While the original formulation used a neural network, follow up work has used voxel grids~\cite{fridovich2022plenoxels, sun2021direct}, hash grids~\cite{muller2022instant}, tri-planes~\cite{chan2022efficient}, primitives~\cite{lombardi2021mixture}, and points~\cite{xu2022point}. These papers all use numerical quadrature to approximately integrate the volume rendering equation.

While NeRF reconstructions are slow to render they can be converted into faster representations, such as triangle meshes~\cite{chen2022mobilenerf,yariv2023bakedsdf,rakotosaona2023nerfmeshing,rojas2023re,Reiser2024SIGGRAPH}, sparse volumes~\cite{garbin2021fastnerf,yu2021plenoctrees,reiser2023merf,duckworth2024smerf}, mesh-volume hybrids~\cite{Wan_2023_CVPR,adaptiveshells2023,turki2023hybridnerf} and 3D Gaussians~\cite{niemeyer2024radsplat}. While these facilitate real-time rendering, creating them is a slow two-step process that first trains a NeRF and then converts it to a faster representation. In the experiments we compare with SMERF~\cite{duckworth2024smerf}, the current state-of-the-art for real-time NeRF rendering.

% Mip-NeRF~\cite{barron2021mip} proposed an integrated positional encoding to fix aliasing issues and enable multi-scale and multi-resolution training, and mip-NeRF 360~\cite{barron2022mip} extended that formulation to enable the reconstruction of the large and unbounded scenes are commonly captured in practice.
% Various methods have proposed NeRF-like variants that store explicit features in various data-structures to accelerate training and rendering~\cite{sun2021direct, muller2022instant, fridovich2022plenoxels, chan2022efficient}, and Zip-NeRF~\cite{barron2023zip} enabled the usage of these accelerated data structures in large unbounded scenes. \jb{this is a lot of prior work on a specific thrust of NeRF that isn't super relevant to the subject of this paper,  I think I'd prefer a more broad overview.}

\myparagraph{Differentiable Point-based Rendering} Like NeRF, 3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} models the scene as a radiance field, but 3DGS represents radiance as a set of Gaussians that are rendered via splatting~\cite{zwicker2001ewa} while NeRF represents radiance using a field that is rendered via ray-marching. 
While 3DGS approximates volume rendering with view-independent opacity and view-dependent radiance, in practice it often yields highly accurate renderings, and these approximations allow it to be trained and rendered quickly, hence its popularity~\cite{chen2024survey}. Since its inception, improvements have been made to 3DGS in terms of aliasing~\cite{geiger2023mip}, camera models~\cite{moenne20243d}, heuristic densification~\cite{bulo2024revising, kheradmand20243d, ververas2024sags, cao2024lightweight, ye2024absgs, yu2024gaussian}, and view-consistency (i.e. ``popping''). 

Popping results from how 3DGS sorts Gaussians once per-frame using their mean, and it has been partly addressed by StopThePop~\cite{radl2024stopthepop} during rasterization and 3DGRT~\cite{moenne20243d} during ray-tracing.  However, StopThePop only approximately sorts per-ray, and both StopThePop and 3DGRT ignore overlap between the primitives. This introduces a blend order artifact shown in Fig.~\ref{fig:popping_train}.  
One way of addressing popping is to compute the volumetric rendering equation with fewer (or zero) approximations. Concurrently with our work, a number of preprints attempt this~\cite{blanc2024, condor2024volumetric, zhou2024unified} but they all achieve significantly slower performance and are subject to significant limitations regarding how much primitives may overlap, or use a different approximation~\cite{wang2022voge}. 
By representing the scene with constant density primitives, our model is able to quickly and exactly compute the volumetric rendering equation, even with unlimited overlap.

% knoll2021path

%Recently, multiple approaches have attempted to address popping issues. So far to date, there have been three approaches. The first is to perform a per ray sort, like StopThePop~\cite{radl2024stopthepop} and 3DGRT~\cite{moenne20243d}. However, as we show in Fig.~\ref{fig:popping_train}, these introduce a blend order artifact that can often be worse than popping. 
%The second approach has been to apply order independent transparency methods~\cite{wyman2016exploring}, as explored in these papers~\cite{blanc2024, condor2024volumetric, zhou2024unified}. These approaches can handle around 10-15 overlapping Gaussians without creating artifacts, but due to the unlimited extent of Gaussians, this ends up being a severe limitation.
%Finally, Zhou et al~\cite{zhou2024unified} do everything on the CPU with as much memory as necessary.

% \emph{Aliasing} has been successfully addressed in the context of pre-filtering and rasterization in Mip-Splatting~\cite{geiger2023mip}. \emph{Gaussian Control} has been extensively studied in multiple papers~\cite{bulo2024revising, kheradmand20243d, ververas2024sags, cao2024lightweight, ye2024absgs}.The \emph{Camera model} of the original rasterization-based 3DGS is limited to pinhole cameras. The ray-tracing alternatives trivially address this issue. 
% % 360-GS~\ref{bai2024360} also shows how this limitation can be lifted for rasterization based methods.
% \yz{I would make clearer that what are the issues we are fixing and what have been solved by others.}

%While the aliasing issues have been addressed~\cite{geiger2023mip} and people are working on the Gaussian control problems~\cite{bulo2024revising, kheradmand20243d}, and camera model limitations~\cite{moenne20243d}.
%In this paper we address the popping issues by introducing a method that can handle unlimited overlap, does not have view dependent density, and can use ray tracing to simulate different camera models. We also introduce some techniques to help control density based primitives that allow density based methods to match the original performance of 3DGS.


%In this paper we show that the benefits of the primitive based representation of 3DGS are not tightly coupled with the rasterization rendering pipeline. We show that you can efficiently optimize and render through ray-casting with modern hardware and this allows for a more flexible and generalizable framework that enables us to incorporate complicated camera models and effects. On top of that ray-tracing allows for easy and accurate analytical integration of the volume equation without any of the approximations that 3DGS is has.

%Although our method is not an Order Independent Transparency (OIT) method, as it uses ray tracing to sort the primitives, it is nonetheless related to OIT methods, as it shares the goal of rendering transparent objects with a deterministic amount of memory \cite{wyman2016exploring}. Our method is able to match the results of the A-Buffer \cite{carpenter1984buffer} algorithm with a deterministic amount of memory.